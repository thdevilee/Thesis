---
title: "KNN simulation synthetic data"
author: "Thomas Devilee"
date: "06/04/2022"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE)
```

Load data and packages
```{r}
library(parallel)
library(doParallel)
library(foreach)
numCores <- detectCores()
registerDoParallel(numCores)
library(readxl)
library(class)
library(globaltest)
library(ggplot2)
library(glmnet)
library(dplyr)
library(MASS)
library(gridExtra)
set.seed(1979418)

### function to read excel files in the current directory in the format "sim_X.xlsx"
ReadData <- function(names){
  out <- vector(mode = "list", length = length(names)) ### initialize empty list with length equal to the number of datasets to be read (names)
  names(out) <- names ### assign list names
  for(i in 1:length(names)){ ### for all datasets to be read
    name <- paste0("sim_", names[i], ".xlsx")
    data <- read_excel(name) ### read excel files
    data <- data[order(data[["y"]]), ] ### order data based on y (label) value (small to big)
    data[["y"]] <- ifelse(data[["y"]] == 0, FALSE, TRUE) ### replace label values with FALSE if 0 and TRUE if 0
    out[[i]] <- data ### save this modified file to the list
  }
  return(out)
}
```

```{r}
data <- ReadData(c("linear", "moons", "circles"))
```

```{r}
### function to create folds implicitly requires the group size per class has to be equal
### if balanced is true (works only for two classes), the labels have to be sorted (low to high) 
create_folds <- function(n_folds, labs, balanced = TRUE, force = FALSE){
  if(balanced){
    if (any(order(as.numeric(labs[["y"]])) != seq(1, nrow(labs)))) stop("Data (labels) is not sorted") ### checks if the above requirments are met
    n1 <- nrow(labs) - sum(as.numeric(labs[["y"]]))
    n2 <- sum(as.numeric(labs[["y"]]))
    reps1 <- n1/n_folds
    reps2 <- n2/n_folds
    if ((!is.int(reps1) | !is.int(reps2)) & !force) stop("Cannot balance folds")
    folds1 <- rep(seq(1, n_folds), ceiling(reps1))[1:n1] ### create n fold indexes for group 1
    folds2 <- rep(seq(1, n_folds), ceiling(reps2))[1:n2] ### create n fold indexes for group 2
    folds <- c(sample(folds1), sample(folds2)) ### randomly assign indexes to each observation in a balanced fashion
  } else{
    folds <- sample(rep(seq(1, n_folds), nrow(labs)/n_folds)) ### randomly assign folds if balanced is false
  }
  return(folds)
}

### function to compute knn ordering matrix for a given distance matrix
knn_mat <- function(dist_mat){
  n <- nrow(dist_mat) ### number of observations
  nn <- (n + 1) - t(apply(dist_mat, MARGIN = 1, FUN = rank)) ### ordering distance per observation
  return(nn)
}

rename_folds <- function(folds){
  digits <- sort(unique(folds)) ### sort all unique fold ids
  for (i in 1:length(digits)){ ### for each element in unique sorted folds
    folds[folds == digits[i]] <- i ### rename to 1, .., k (k = length(unique sorted folds))
  }
  return(folds) 
}

InvPropWeight <- function(P, y){
  data <- data.frame(P, y = y)
  Z <- model.matrix(y ~ 1, data = data)
  X <- model.matrix(y ~ 0 + ., data = data)
  m <- ncol(Z)
  n <- nrow(Z)
  y <- residuals(lm(y ~ 1, data = data))
  sumyy <- sum(y*y)
  X <- X - Z %*% solve(crossprod(Z), crossprod(Z, X))
  csm <- colSums(X*X)
  X[,csm < max(csm)*1e-14] <- 0
  xy <- crossprod(X, y)
  S <- sum(xy * xy) / sumyy
  if (sumyy == 0) S <- 0
  lams <- eigen(crossprod(X), symmetric = TRUE, only.values=TRUE)$values
  if (length(lams) < n) lams <- c(lams, numeric(n-length(lams)))
  lams[1:(n-m)] <- lams[1:(n-m)] - S
  tr.term <- crossprod(X)
  var.num <- 2*sum(tr.term*tr.term)
  mu.num <- sum(lams) + (n-m)*S
  mu.den <- n-m
  var.den <- 2*(n-m)
  cov.term <- 2*mu.num
  VarS <- (mu.num^2/mu.den^2) * (var.num/(mu.num^2)  + var.den/(mu.den^2) - 2*cov.term/(mu.num*mu.den))
  weight <- 1/VarS
  if (is.nan(weight) | weight > 10^6) weight <- 0
  return(weight)
}

XXT <- function(ord_mat, weights = NA){
  weighting <- TRUE ### weighting true by default
  k <- dim(ord_mat)[1] ### number of observations/partition matrices
  if(all(is.na(weights))){ ### if no weights specified
    weighting <- FALSE ### set weighting to FALSE
    weights <- rep(1, k) ### set al weights to 1
  }
  if(weighting & k != length(weights)) stop("Incorrect dimensions weights") ### check if the dimensions of the weights match the number of partition matrices
  
  weights <- cumsum(rev(weights)^2) ### compute cumulative squared weights
  out <- matrix(data = NA, nrow = k, ncol = k) ### initialize output matrix
  diag(out) <- sum(weights) ### compute diagonal
  for(i in 1:(k - 1)){ ### compute all elements in the upper triangle (without diagonal) of the output matrix
    for(j in (i + 1):k){
      out[i, j] <- sum(weights[pmin(ord_mat[i,], ord_mat[j,])])
    }
  }
  out[lower.tri(out)] <- t(out)[lower.tri(out)] ### reflect upper triangle onto lower triangle
  return(out)
}

### function to compute p-values based on the partition matrix (for 1, ..., k neighbours and the overall test) for reps replications
power_knn <- function(data, reps){
  p_vals <- matrix(data = NA, nrow = reps, ncol = n) ### initialize empty matrix for p-values from global test 
  
  for (i in 1:reps){ ### for all replications
    X <- vector(mode = "list", length = n) ### initialize empty list of length n to accomodate all k partition matrices
    sub_data <- data[folds == i, ] ### select data in fold i
    sub_X <- as.matrix(sub_data[, !names(sub_data) %in% "y"])
    dist_mat <- as.matrix(dist(sub_X, diag = TRUE, upper = TRUE)) ### compute distance matrix
    ord_mat <- knn_mat(dist_mat) ### compute ordering mat
    for (j in 1:n){ ### for all 1, ..., k neighbours
      X[[j]] <- ifelse(ord_mat > (n - j), 1, 0) ### compute partition matrix and save in list
      p_vals[i, j] <- gt(y ~ 1, y ~ ., data = data.frame(X[[j]], y = sub_data["y"]))@result[1] ### compute gt statistic based on partition matrix
    }
  }
  return(p_vals) ### return a matrix with p-values with reps rows and n + 1 columns, where the last column contain p-values from the overall gt
}

is.int <- function(x) x%%1 == 0

EmpericalPower <- function(data){
  out <- apply(data, 2, function(x) mean(x < 0.05))
  return(out)
}
```

```{r}
### All functions below require specification of  the regressors (regs) as data frame and label (labs) as data frame. Some functions also require the fold id (folds) as vector

### function to performed nested knn based on a specified number of neighbours
nestedcv.knn <- function(folds, regs, labs){
  n_folds <- max(folds) ### total number of folds
  res_outer <- numeric(n_folds) ### initialize outerloop output
  fold_id <- seq(1, n_folds) ### create fold numbers for innerloop
  n_trin <- ((n_folds - 2)/n_folds)*nrow(regs)
  neighbours <- round(seq(1, n_trin, length.out = round(sqrt(n_trin))))
  for(i in 1:n_folds){  ### for all folds
    res_inner <- matrix(NA, nrow = n_folds - 1, ncol = length(neighbours)) ### initialize innerloop output matrix
    for(j in 1:(n_folds - 1)){ ### for all folds - 1 (excluding one outer validations set)
      for(k in 1:length(neighbours)){ ### for every neighbour
        indx <- fold_id[fold_id != i]  ### exclude outer validation set
        tmp <- class::knn(regs[(folds %in% indx[-j]),], regs[(folds == indx[j]),], labs[(folds %in% indx[-j]), , drop = TRUE], k = neighbours[k]) ### train on n_folds - 2
        res_inner[j, k] <- mean(tmp != labs[folds == indx[j], , drop = TRUE]) ### validate on innerloop validation set
      }
    }
    res_inner <- apply(res_inner, MARGIN = 2, mean) ### compute average misclassification rate over the innerloop folds
    preds <- class::knn(regs[folds != i,], regs[folds == i,], labs[folds != i, , drop = TRUE], k = neighbours[which.min(res_inner)]) ### train on n_folds - 1 for the k with the smallest misclassification rate
    res_outer[i] <- mean(preds == labs[folds == i, , drop = TRUE])  ### compute average accuracy on outer validation set
  }
  return(mean(res_outer))
}
### function to compute perform nested ridge
nestedcv.ridge <- function(folds, regs, labs){
  n_folds <- max(folds) ### number of folds
  res_outer <- numeric(n_folds) ### initialize output 
  for(i in 1:n_folds){ 
    log_indx <- folds != i ### create logical index for subset of the data
    regs_sub <- as.matrix(regs[log_indx, ]) ### select regressor and put in right format
    labs_sub <- labs[log_indx, , drop = TRUE] ### select labels
    folds_sub <- rename_folds(folds[log_indx]) ### rename folds to 1, ... k - 1 (required for cv.glment)
    res_cv <- glmnet::cv.glmnet(x = regs_sub, y = labs_sub, type.measure = "mse", foldid = folds_sub, alpha = 0) ### compute optimal lambda
    preds <- predict(res_cv, as.matrix(regs[folds == i,]), s = "lambda.min") >= 0.5 ### make predictions for the validation set from a model with the optimal lambda
    res_outer[i] <- mean(preds == labs[folds == i, , drop = TRUE]) ### compute average accuracy on outer validation set
  }
  return(mean(res_outer))
}
### function to compute linear global test
linear.gt <- function(regs, labs){
  res <- gt(y~1, y~., data = data.frame(regs, y = labs)) ### linear global test
  return(res@result[1])  ### return p-value
}

```

```{r}
### function to perform unweighted and inverse weighted overall global test
overall.gt <- function(regs, labs, invweighted = FALSE){
  n <- nrow(regs) ### compute number of observations
  dist_mat <- as.matrix(dist(regs, diag = TRUE, upper = TRUE)) ### compute distance matrix
  ord_mat <- knn_mat(dist_mat) ### compute ordering matrix
  weights <- numeric(n) ### initialize weight vector
  if (invweighted){ ### if inverse weights
    for(i in 1:n){ ### for all 1, ..., k neighbours
      P <- ifelse(ord_mat > (n - i), 1, 0) ### compute partition matrix
      weights[i] <- InvPropWeight(P, labs) ### weight according to inverse of variance of partition matrix
    }
  }
  X <- t(chol(XXT(ord_mat = ord_mat))) ### perform cholesky decomosition on XXT to increase computation performance
  res <- globaltest::gt(y ~ 1, y ~ ., data = data.frame(X, y = labs))@result[1] ### perform global test on cholesky decomposition
  if(invweighted){ ### if inverse weights
    X_iw <- t(chol(XXT(ord_mat = ord_mat, weights = weights))) ### perform cholesky decomosition on weighted XXT to increase computation performance
    return(XXT(ord_mat = ord_mat, weights = weights))
    res <- cbind(res, globaltest::gt(y ~ 1, y ~ ., data = data.frame(X_iw, y = labs))@result[1]) #### perform global test on inversely weighted partition matrices
  }
  return(res)
}
tmp10 <- overall.gt(regs = tmp1[[1]], labs = tmp1[[2]], invweighted = TRUE)
```


```{r}
### function to perform global test on a partition matrix which corresponds to the (smallest) fraction of labels times the number of observations (rounded)
prop.knn_gt <- function(regs, labs){
  prop <- mean(labs[["y"]]) ### compute fraction of labels
  prop <- min(prop, 1 - prop) ### select the smallest fraction
  dist_mat <- as.matrix(dist(regs, diag = TRUE, upper = TRUE)) ### compute distance matrix
  ord_mat <- knn_mat(dist_mat) ### compute ordering mat
  P <- ifelse(ord_mat > nrow(regs) - prop*nrow(regs), 1, 0) ### compute partition matrix corresponding to the smallest fraction of labels
  res <- gt(y ~ 1, y ~ ., data = cbind(P, labs))@result[1] ### compute gt statistic based on partition matrix
  return(res)
}

### function to perform the global test on the partition matrix of the validation set for the value of k which corresponds to the smallest p-value in the training set (splits data in two)
pval.knn_gt <- function(regs, labs, balanced = TRUE, force = FALSE){
  i <- 1 ### arbitrarily choose a validation fold
  folds <- create_folds(n_folds = 2, labs = labs, balanced = balanced, force = force) ### create folds such that the groups are balanced
  n_train <- sum(folds != i) ### compute the number of observations in each folds
  neighbours <- round(seq(2, n_train - 1, length.out = round(sqrt(n_train))))
  dist_mat <- as.matrix(dist(regs[folds != i, ], diag = TRUE, upper = TRUE)) ### compute distance matrix
  ord_mat <- knn_mat(dist_mat) ### compute ordering mat
  p_vals <- numeric(length(neighbours)) ### initialize vector for p-values
  p_vals[1] <- 1 ### skip p-value for partition matrix which corresponds to k = 1
  for (j in 1:length(neighbours)){ ### for all 2, ..., k neighbours
    P <- ifelse(ord_mat > (neighbours[j] - j), 1, 0) ### compute partition matrix and save in list
    p_vals[j] <- log(globaltest::gt(y ~ 1, y ~ ., data = cbind(P, labs[folds != i, ]))@result[1]) ### compute gt statistic based on partition matrix
    }
    optimal_k <- which.min(p_vals) ### select k which corresponds to smallest p-value
    n_test <- sum(folds == i) ### compute number of observations in validation set
    dist_mat <- as.matrix(dist(regs[folds == i, ], diag = TRUE, upper = TRUE)) ### compute distance matrix
    ord_mat <- knn_mat(dist_mat) ### compute ordering mat
    P <- ifelse(ord_mat > (n_test - optimal_k), 1, 0) ### compute partition matrix for optimal k
    res <- gt(y ~ 1, y ~ ., data = cbind(P, labs[folds == i, ]))@result[1] ### perform global test
  return(res)
}

overall.gt <- function(regs, labs, invweighted = FALSE){
  n <- nrow(regs) ### compute number of observations
  dist_mat <- as.matrix(dist(regs, diag = TRUE, upper = TRUE)) ### compute distance matrix
  ord_mat <- knn_mat(dist_mat) ### compute ordering matrix
  P <- P_iw <- vector(mode = "list", length = n)
  for(i in 1:n){ ### for all 1, ..., k neighbours
    P[[i]] <- ifelse(ord_mat > (n - i), 1, 0) ### compute partition matrix
    if(invweighted){
      weight <- InvPropWeight(P[[i]], labs) ### weight according to inverse of variance of partition matrix
      P_iw[[i]] <- P[[i]] * weight
    }
  }
  X <- do.call(cbind, P)
  res <- globaltest::gt(y ~ 1, y ~ ., data = data.frame(crossprod(t(X)), y = labs))@result[1] ### perform global test on cholesky decomposition
  if(invweighted){ ### if inverse weights
    X_iw <- do.call(cbind, P_iw) ### perform cholesky decomosition on weighted XXT to increase computation performance
    res <- cbind(res, globaltest::gt(y ~ 1, y ~ ., data = data.frame(crossprod(t(X_iw)), y = labs))@result[1]) #### perform global test on inversely weighted partition matrices
    return(crossprod(t(X_iw)))
  }
  return(res)
}
tmp11 <- overall.gt(regs = tmp1[[1]], labs = tmp1[[2]], invweighted = TRUE)
```

```{r}
pckgs <- as.vector(lsf.str())
  
SimExp <- function(data, n_reps, pckgs, indx = NA, n_perms = 100, ...){
  labs <- data[, "y"] ### extract labels
  regs <- data[, !names(data) %in% "y"] ### extract predictors
  if(all(is.na(indx))){ ### if no weights specified
    indx <- create_folds(n_folds = n_reps, labs = labs, ...) ### compute folds equal to the number of replications
  }
  res <- matrix(data = NA, nrow = n_reps, ncol = 7) ### initialize output matrix
  colnames(res) <- c("linear.gt", "nestedcv.ridge", "nestedcv.knn", "prop.knn_gt", "pval.knn_gt", "overall.gt", "invoverall.gt") ### assign names to the output matrix
  
  for(i in 1:max(indx)){ ### for each folds
    sub_regs <- regs[indx == i,] ### select regressor subset
    sub_labs <- labs[indx == i,] ### select label subset
    folds <- create_folds(n_folds = 5, labs = sub_labs, ...) ### create folds for functions that require cross validation
    res[i, 1] <- linear.gt(regs = sub_regs, labs = sub_labs) ### compute p-value of the linear global test
    ridge_stat <- nestedcv.ridge(folds = folds, regs = sub_regs, labs = sub_labs) ### compute test statistic for the nested ridge
    knn_stat <- nestedcv.knn(folds = folds, regs = sub_regs, labs = sub_labs) ### compute test statistic for the nested knn
    res[i, 4] <- prop.knn_gt(regs = sub_regs, labs = sub_labs) ### compute p-value for the global test on the partition matrix based on the label proportion
    res[i, 5] <- pval.knn_gt(regs = sub_regs, labs = sub_labs, ...) ### compute p-value for the global test based on the value of k which has the smallest p-value in the training set
    res[i, c(6, 7)] <- overall.gt(regs = sub_regs, labs = sub_labs, invweighted = TRUE) ### compute unweighted and inversely weighted overall gloval test
    
    perm_mat <- foreach(j = 1:n_perms, .combine = "cbind", .export = pckgs) %dopar% { ### for n_perms replications
      perm_indx <- sample(nrow(sub_labs)) ### permutation index
      perm_regs <- sub_regs[perm_indx, ] ### permute regressors
      perm_folds <- create_folds(n_folds = 5, labs = sub_labs, ...) ### compute folds
      perm_ridge <- nestedcv.ridge(folds = perm_folds, regs = perm_regs, labs = sub_labs) ### compute (permuted) test statistic for the nested ridge
      perm_knn <- nestedcv.knn(folds = perm_folds, regs = perm_regs, labs = sub_labs) ### compute (permuted) test statistic for the nested knn
      matrix(c(perm_ridge, perm_knn), nrow = 2) ### return permuted test statistics for both tests
    }
    res[i, 2] <- 1- mean(ridge_stat >= perm_mat[1, ]) ### compute emperical p-value for nested ridge
    res[i, 3] <- 1- mean(knn_stat >= perm_mat[2, ]) ### compute emperical p-value for nested knn
  }
  return(res)
}
```

```{r}
ggplot(data[["circles"]][c(1:20, 50001:50020), ], aes(x = x1, y = x2, colour = y)) + geom_point()

SubsampData <- function(data, n_reps, sampsize, n = 50000, ...){
  indx <- seq(1, n)
  sub_data <- rbind(data[sample(indx, sampsize*n_reps/2), ], data[sample(indx, sampsize*n_reps/2) + n, ]) 
  out <- SimExp(n_reps = n_reps, data = sub_data, ...)
  out <- EmpericalPower(out)
  return(out)
}
SubsampData(data[["circles"]], sampsize = 40, pckgs = pckgs, n_reps = 100)
```

```{r}
ggplot(data[["linear"]][c(1:20, 50001:50020), ], aes(x = x1, y = x2, colour = y)) + geom_point()

SubsampData(data[["linear"]], sampsize = 40, pckgs = pckgs, n_reps = 100)
```

```{r}
### function to take a sample that follows a checkerboard pattern. Here p represents the number of dimensions, n the number of observations per position on the checkerboard, n_clusts the number of clusters (use an uneven number, an even number gives a stripes pattern)
CheckerSamp <- function(n, n_clusts, p = 2, c = 1){
  data <- matrix(runif((n_clusts^2)*n * p), nrow = (n_clusts^2)*n, ncol = p) ### draw from uniform
  shift <- expand.grid(seq(0, n_clusts - 1), seq(0, n_clusts - 1)) ### compute shift
  shift <- do.call(rbind, replicate(n, shift, simplify = FALSE))*c ### replicate shift such that each cluster has n obs
  labs <- rep(c(FALSE, TRUE), ceiling(n_clusts^2/2))[1:(n_clusts^2)] ### generate labs
  labs <- rep(labs, n) ### replicate labs such that every obs has a label according to its cluster
  data <- data + shift ### compute shifted regressors
  data <- cbind(data, labs) ### combine regressors and labels into matrix
  names(data) <- c("x1", "x2", "y") ### rename columns
  indx <- order(data[["y"]])
  data <- data[indx, ]
  return(data)
}
ggplot(CheckerSamp(p = 2, n = 5, n_clusts = 3, c = 1), aes(x = x1, y = x2, colour = y)) + geom_point()
```

```{r}
CheckerSim <- function(n_reps, n, n_clusts, ...){
  indx <- c(length(n) > 1, length(n_clusts) > 1)
  if(sum(as.numeric(indx)) > 1) stop("Only one parameter can have multiple values") ### only allow for one parameter to take on multiple values
  var_name <- c("n", "n_clusts")[indx]
  params <- cbind(n, n_clusts) ### parameter space
  out <- vector(mode = "list", length = nrow(params))
  for (i in 1:nrow(params)){
    sampsize <- params[[i, "n_clusts"]]^2 * params[[i, "n"]]
    indx <- as.vector(sapply(1:n_reps, function(x, sampsize) rep(x, sampsize), sampsize = sampsize))
    data <- lapply(1:n_reps, FUN = function(x, n, n_clusts) CheckerSamp(n = n, n_clusts = n_clusts),
                   n = params[[i, "n"]], n_clusts = params[[i, "n_clusts"]])
    data <- as_tibble(do.call(rbind, data))
    tmp <- SimExp(n_reps = n_reps, data = data, indx = indx, ...)
    out[[i]] <- EmpericalPower(tmp)
  }
  out <- do.call(rbind, out)
  out <- data.frame(out, params[, var_name])
  colnames(out)[ncol(out)] <- var_name
  out <- pivot_longer(out, !last_col(), names_to = "method", values_to = "RF")
  out <- ggplot(data = out, aes(x = p, y = RF, colour = method)) + geom_line(linetype = "dashed") + geom_point()
  return(out)
}
tmp <- CheckerSim(n_reps = 10, n = c(3), n_clusts = 3, pckgs = pckgs, force = TRUE)
```


```{r}
CheckerSim <- function(n_reps, n, n_clusts, ...){
  ellip <- list(...)
  sampsize <- n_clusts^2*n
  indx <- as.vector(sapply(1:n_reps, function(x, sampsize) rep(x, sampsize), list(sampsize = sampsize)))
  data <- lapply(1:n_reps, FUN = function(x) CheckerSamp(n = n, n_clusts = n_clusts))
  data <- as_tibble(do.call(rbind, data))
  return(list(data, indx))
  out <- SimExp(n_reps = n_reps, data = data, indx = indx, ...)
  out <- EmpericalPower(out)
  return(out)
}
tmp <- CheckerSim(n_reps = 3, n = 5, n_clusts = 3, pckgs = pckgs, force = TRUE)
```

```{r}
LDASamp <- function(n_samples, p, effect){
  inv_vcov <- solve(diag(p)) ### inverse of the covariance matrix (mahalanobis distance) (must be identity in this case)
  e_vec <- rep(1,p) ### e vector (mahalanobis distance)
  c <- sqrt( effect / (n_samples * (e_vec %*% inv_vcov %*% e_vec)) ) ### compute constant as a function of the effect size (mahalanobis distance)
  shift <- matrix(effect*c, nrow=n_samples/2, ncol=p, byrow = TRUE) ### compute the shift for one class
  labs <- sample(rep(c(TRUE, FALSE), n_samples/2)) ### randomly sample labels (balanced)
  noise <- matrix(rnorm(n_samples*p), ncol = p, nrow = n_samples) ### generate regressor (independent standard normal)
  noise[labs,] <- noise[labs,] + shift ### compute shifted regressors 
  indx <- order(labs) ### sort labels (required for power_knn func)
  labs <- labs[indx] ### order labels
  noise <- noise[indx, ] ### order regressor in the same sequence
  output <- data.frame(noise, labs) ### concatenate regressors and labels into dataframe
  names(output) <- c(paste0("x", 1:p), "y") ### change column names
  return(output)
}
```

```{r}
LDASim <- function(sampsize, n_reps, effect, p, pckgs, ...){
  indx <- c(length(p) > 1, length(effect) > 1, length(sampsize) > 1)
  if(sum(as.numeric(indx)) > 1) stop("Only one parameter can have multiple values") ### only allow for one parameter to take on multiple values
  var_name <- c("p", "effect", "sampsize")[indx]
  params <- cbind(p, effect, sampsize) ### parameter space
  out <- vector(mode = "list", length = nrow(params))
  for (i in 1:nrow(params)){
    print(i)
    data <- as_tibble(LDASamp(n_samples = params[i, "sampsize"] * n_reps, p = params[i, "p"], effect = params[i, "effect"]))
    tmp <- SimExp(n_reps = n_reps, data = data, pckgs = pckgs, ...)
    out[[i]] <- EmpericalPower(tmp)
  }
  out <- do.call(rbind, out)
  out <- data.frame(out, params[, var_name])
  colnames(out)[ncol(out)] <- var_name
  out <- pivot_longer(out, !last_col(), names_to = "method", values_to = "RF")
  out <- ggplot(data = out, aes(x = p, y = RF, colour = method)) + geom_line(linetype = "dashed") + geom_point()
  return(out)
}
tmp <- LDASim(sampsize = 40, n_reps = 100, effect = 25, p = c(2, 10, 25, 50, 100), pckgs = pckgs)
```


```{r}
regs <- data[["circles"]][c(1:50, 50001:50050), c("x1", "x2")]
labs <- data[["circles"]][c(1:50, 50001:50050), c("y")]
folds <- create_folds(n_folds = 5, labs = labs)
```


